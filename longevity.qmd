---
title: "Mid-life predictors of Longevity"
project:
  type: website
  output-dir: docs
execute: 
  echo: true
format: 
  html:
    code-fold: show
    code-summary: "Show the code"
bibliography: references.bib
---

```{r}
#| label: load-packages-defs
#| message: false
#| include: false
library(tidyverse)
library(tidymodels)
library(parallel)
library(doParallel)
library(probably)
tidymodels_prefer()

cal_plot <- function(final_fit, train_fit, split = c("train", "test","stack"), plat = FALSE) {
  if (split == "train") {
    if (plat) {
      df <- train_fit %>%
        collect_predictions() %>% 
        cal_apply(cal_estimate_logistic(train_fit))
    } else {
      df <- train_fit %>% collect_predictions()
    } 
  } else if (split == "test") {
    if (plat) {
      df <- final_fit %>%
        collect_predictions () %>%
        cal_apply(cal_estimate_logistic(train_fit))
    } else {
      df <- final_fit %>%
        collect_predictions()
    }
  } else if (split == "stack") {
    df <- final_fit
  }
  
  lm_cal <- lm(outcome ~ .pred_centenarian, data = df %>%
                 mutate(outcome = as.numeric(outcome == "centenarian")))
  
  plot_txt <- tibble(
    txt = paste0("Intercept ", round(lm_cal$coefficients[1],2),
                 "\nslope ", round(lm_cal$coefficients[2],2)))
  
  title <- ifelse(split == "train","Training set","Testing set")
  
  df %>%
    mutate(outcome = as.numeric(outcome == "centenarian")) %>%
    ggplot(aes(.pred_centenarian, outcome)) +
    geom_rug(sides = "tb", color = "grey") +
    geom_smooth(method = "loess",
                color = "black",
                se = TRUE, fullrange = TRUE) +
    geom_abline(intercept = 0, slope = 1, linetype = "longdash") +
    geom_text(data = plot_txt, aes(x = 0.1, y = 0.95, label = txt)) +
    labs(x = "Predicted risk",
         y = "Observed proportion") +
    theme_bw() +
    coord_equal(xlim = c(0,1), ylim = c(0,1)) +
    theme(
      legend.position = "bottom",
      legend.title = element_blank()) + 
    ggtitle(title) -> plot
  
  return(plot)
}

thresh_other <- 0.05
thresh_corr <- 0.9
ctl_grid <- control_resamples(save_pred = TRUE, 
                              save_workflow = TRUE, 
                              parallel_over = 'everything')

load("raw_data/models.RData")
```

# Data exploration

This study uses a well known database of `r nrow(ml_df)` men recruited in 1963 and were followed up for 5 years to investigate ischemic heart disease incidence [@medalie1973]. The goal is to predict longevity using mid-life predictors.

## Whats inside?

This data frame contains `r ncol(ml_df)-1` variables:

1.  Demographics data (`r sum(str_starts(colnames(ml_df),"dmg"))`)

2.  Medical data including medical history, laboratory tests and measurements (`r sum(str_starts(colnames(ml_df),"med"))+sum(str_starts(colnames(ml_df),"lab"))+sum(str_starts(colnames(ml_df),"comp"))`)

3.  Daily physical activity assessment (`r sum(str_starts(colnames(ml_df),"physical"))`)

4.  Work related data (e.g. satisfaction, income etc.) (`r sum(str_starts(colnames(ml_df),"work"))`)

5.  Family relations (`r sum(str_starts(colnames(ml_df),"family"))`)

6.  Diet habits (`r sum(str_starts(colnames(ml_df),"diet"))`)

7.  Outcome - reaching to the age of 95, prevalence of `r round(100*sum(ml_df$outcome == "centenarian")/nrow(ml_df),2)` %

## What are we going to do

1.  prepare our data for ML analysis

2.  create specifications for the different models

3.  hyper-tune parameters

4.  examine results

## ML data preparation

```{r}
#| label: split data
#| echo: true
set.seed(1234) #<1>
df_split <- initial_split(ml_df, strata = outcome) #<2>
df_train <- training(df_split) #<3>
df_test <- testing(df_split) #<3>
df_train_cv <- vfold_cv(df_train, v = 10, strata = outcome) #<4>

```

1.  Set a seed, for constant results
2.  Make the split object for training / testing
3.  Create the train / test data
4.  Create a cross validation train set

# LASSO

## Specify the model

```{r}
#| label: lasso-spec-prep
#| eval: false
lasso_rec <- recipe(outcome ~ ., data = df_train) %>%
  update_role(id_nivdaki, new_role = "ID") %>%
  step_impute_knn(all_predictors(), neighbors = 3) %>%
  step_poly(all_numeric_predictors(),degree = 3) %>%
  step_date(all_date_predictors(), keep_original_cols = FALSE, features = "month")%>%
  step_other(all_nominal_predictors(), threshold = thresh_other, other = "other_combined") %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = thresh_corr, method = "spearman") %>%
  step_normalize(all_numeric_predictors())

lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode('classification')

lasso_wf <- workflow() %>%
  add_recipe(lasso_rec) %>% 
  add_model(lasso_spec)

lasso_prep <- lasso_rec %>%        # for later use in SHAP
  prep(strings_as_factors = FALSE,
       log_changes = TRUE,
       verbose = TRUE)
```

```{r}
#| echo: false
lasso_prep
```

## Hypertune parameters

```{r}
#| label: lasso-hyper-parameter
#| eval: false
set.seed(2020)

lambda_grid <- grid_regular(penalty(), levels = 1000)

all_cores <- parallel::detectCores(logical = TRUE)
cl <- makeCluster(all_cores-2)
registerDoParallel(cl)

lasso_res <- tune_grid(
  lasso_wf,
  resamples = df_train_cv,
  grid = lambda_grid,
  control = ctl_grid
)

stopCluster(cl)
```

```{r}
#| label: lasso-res
lasso_res %>%
  autoplot()

lasso_res %>%
  show_best("roc_auc")
```

## Choose and fit best model

```{r}
#| label: lasso-fit
#| eval: false
lasso_best_auc <- lasso_res %>%
  select_best("roc_auc")

final_lasso <- finalize_workflow(
  lasso_wf,
  lasso_best_auc
)

final_lasso_fit <- final_lasso %>%
  last_fit(df_split)

collect_metrics(final_lasso_fit)
```

```{r}
#| echo: false
collect_metrics(final_lasso_fit)
```

# XGBoost

## Specify the model

```{r}
#| label: xgb-spec-prep
#| eval: false
xgb_rec <- recipe(outcome ~ ., data = df_train) %>%
  update_role(id_nivdaki, new_role = "ID") %>%
  step_date(all_date_predictors(), keep_original_cols = FALSE, features = "month") %>%
  step_integer(all_ordered_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = thresh_other, other = "other_combined") %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = thresh_corr, method = "spearman")

xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(), min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(), mtry = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_wf <- workflow() %>%
  add_recipe(xgb_rec) %>% 
  add_model(xgb_spec)

xgb_prep <- xgb_rec %>%
  prep(strings_as_factors = FALSE,
       log_changes = TRUE,
       verbose = TRUE)

xgb_bake <- bake(xgb_prep, df_train)
```

```{r}
#| echo: false
xgb_prep
```

## Hypertune parameters

```{r}
#| label: xgb-hyper-parameter 
#| eval: false
set.seed(2020)

xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 500
)

all_cores <- parallel::detectCores(logical = TRUE)
cl <- makeCluster(all_cores-2)
registerDoParallel(cl)

xgb_res <- tune_grid(
  xgb_wf,
  resamples = df_train_cv,
  grid = xgb_grid,
  control = ctl_grid
)

stopCluster(cl)
```

```{r}
#| label: xgb-res 
xgb_res %>%
  autoplot()

xgb_res %>%
  show_best("roc_auc")
```

## Choose and fit best model

```{r}
#| label: xgb-fit
#| eval: false
xgb_best_auc <- xgb_res %>%
  select_best("roc_auc")

final_xgb <- finalize_workflow(
  xgb_wf,
  xgb_best_auc
)

final_xgb_fit <- final_xgb %>%
  last_fit(df_split)

collect_metrics(final_xgb_fit)
```

```{r}
#| echo: false
collect_metrics(final_xgb_fit)
```

# Models assessment

## ROC & PR plots

@fig-ROC presents the ROC curves of the models

```{r}
#| label: fig-ROC
#| fig-cap: "ROC & PR curves for the models"
#| fig-subcap: 
#| - "ROC"
#| - "PR"
#| layout-ncol: 2
#| column: page

lasso_roc <- final_lasso_fit %>%
  collect_predictions() %>%
  roc_curve(outcome, .pred_centenarian) %>%
  mutate(model = paste("LASSO", round(collect_metrics(final_lasso_fit)[2,3],3)))

xgb_roc <- final_xgb_fit %>%
  collect_predictions() %>%
  roc_curve(outcome, .pred_centenarian) %>%
  mutate(model = paste("XGBoost", round(collect_metrics(final_xgb_fit)[2,3],3)))

lasso_pr <- final_lasso_fit %>%
  collect_predictions() %>%
  pr_curve(outcome, .pred_centenarian) %>%
  mutate(model = paste("LASSO", round(pr_auc(data = final_lasso_fit %>%
                                               collect_predictions(),
                                             truth = outcome,
                                             .pred_centenarian)[1,3],3)))

xgb_pr <- final_xgb_fit %>%
  collect_predictions() %>%
  pr_curve(outcome, .pred_centenarian) %>%
  mutate(model = paste("XGBoost", round(pr_auc(data = final_xgb_fit %>%
                                                 collect_predictions(),
                                               truth = outcome,
                                               .pred_centenarian)[1,3],3)))

rbind(lasso_roc, xgb_roc) %>% #
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    linewidth = 1.2
  ) +
  geom_path() +
  coord_equal() +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())

rbind(lasso_pr, xgb_pr) %>% #
  filter(!is.infinite(.threshold),
         !is.infinite(precision)) %>%
  ggplot(aes(x = recall, y = precision, color = model)) +
  geom_path() +
  coord_equal(ratio = 1/0.4) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

## Calibration plots

In @fig-cal presented calibration plots for the lasso model @fig-cal-1 and XGBoost @fig-cal-2 after Platt scaling

```{r}
#| label: fig-cal
#| fig-cap: "Calibration plots"
#| fig-subcap: 
#| - "LASSO"
#| - "XGBoost"
#| layout-ncol: 2
#| column: page
#| message: false
#| echo: false

cal_plot(final_lasso_fit,"lasso_train_fit", "test", FALSE)
cal_plot(final_xgb_fit,"xgb_train_fit", "test", FALSE)
```
